{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO2IgPs0wVHzR0IriaEfxPj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["pip install nltk"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oMRJAq-alVlP","executionInfo":{"status":"ok","timestamp":1740310418724,"user_tz":-330,"elapsed":2458,"user":{"displayName":"Parth Kumbhar","userId":"05674009928445710511"}},"outputId":"010811f9-fbfa-4734-d7e0-109001a2a8fb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"]}]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"drEJjji0lcbf","executionInfo":{"status":"ok","timestamp":1740721869696,"user_tz":-330,"elapsed":5103,"user":{"displayName":"Parth Kumbhar","userId":"05674009928445710511"}},"outputId":"dfbf4d38-3330-454f-9f64-9482608baadc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"0h3zDoMyjQW3","executionInfo":{"status":"ok","timestamp":1741586893182,"user_tz":-330,"elapsed":7775,"user":{"displayName":"Parth Kumbhar","userId":"05674009928445710511"}}},"outputs":[],"source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","from sklearn.feature_extraction.text import TfidfVectorizer"]},{"cell_type":"code","source":["nltk.download('punkt_tab')\n","nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PUQWpVABkee1","executionInfo":{"status":"ok","timestamp":1741586898631,"user_tz":-330,"elapsed":327,"user":{"displayName":"Parth Kumbhar","userId":"05674009928445710511"}},"outputId":"0886a80c-9c34-443a-b867-e754fbc66be2"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["documents=[\n","    'This is sample document for testing the text similarity',\n","    'we will use nltk for computing similarity of text',\n","    'NLTK is powerful library for natural language processing'\n","]"],"metadata":{"id":"dCaohZ4lk8JO","executionInfo":{"status":"ok","timestamp":1741586899843,"user_tz":-330,"elapsed":4,"user":{"displayName":"Parth Kumbhar","userId":"05674009928445710511"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def preprocess_and_stem(text):\n","    \"\"\"Preprocess a single text document and apply stemming.\"\"\"\n","\n","    stemmer = PorterStemmer()\n","    stop_words = set(stopwords.words('english'))\n","\n","    # Tokenize the text\n","    tokens = word_tokenize(text)\n","\n","    # Preprocess: Lowercase, remove punctuation, remove stopwords, apply stemming\n","    processed_tokens = [\n","        stemmer.stem(word.lower())\n","        for word in tokens\n","        if word.isalpha() and word.lower() not in stop_words\n","    ]\n","\n","    return processed_tokens\n","def preprocess_documents(documents):\n","    \"\"\"Preprocess a list of text documents.\"\"\"\n","    return [preprocess_and_stem(doc) for doc in documents]\n"],"metadata":{"id":"f50aX-fFm--t","executionInfo":{"status":"ok","timestamp":1741586903154,"user_tz":-330,"elapsed":14,"user":{"displayName":"Parth Kumbhar","userId":"05674009928445710511"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["preprocessed_doc = preprocess_documents(documents)\n","\n","for i, doc in enumerate(preprocessed_doc):\n","    print(f\"Document {i+1}: {doc}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IAA2F-jHnGC0","executionInfo":{"status":"ok","timestamp":1741586905242,"user_tz":-330,"elapsed":44,"user":{"displayName":"Parth Kumbhar","userId":"05674009928445710511"}},"outputId":"a75f1d50-f7be-4348-b9ff-cec57d10faa0"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Document 1: ['sampl', 'document', 'test', 'text', 'similar']\n","Document 2: ['use', 'nltk', 'comput', 'similar', 'text']\n","Document 3: ['nltk', 'power', 'librari', 'natur', 'languag', 'process']\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Convert tokenized words back into string format for TfidfVectorizer\n","processed_documents_str = [\" \".join(doc) for doc in preprocessed_doc]\n","print(processed_documents_str)\n","# Initialize and fit TF-IDF Vectorizer\n","tfidf_vectorizer = TfidfVectorizer()\n","tfidf_matrix = tfidf_vectorizer.fit_transform(processed_documents_str)\n","\n","# Get feature names (words)\n","feature_names = tfidf_vectorizer.get_feature_names_out()\n","\n","# Converting to dataframe fr readability\n","tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n","\n","# Display the TF-IDF matrix\n","print(\"TF-IDF Matrix:\")\n","print(tfidf_df)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_jve-c4rmmr1","executionInfo":{"status":"ok","timestamp":1741586906289,"user_tz":-330,"elapsed":48,"user":{"displayName":"Parth Kumbhar","userId":"05674009928445710511"}},"outputId":"be2ab6af-6088-49c5-830f-950207bd0d95"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["['sampl document test text similar', 'use nltk comput similar text', 'nltk power librari natur languag process']\n","TF-IDF Matrix:\n","    comput  document   languag   librari     natur      nltk     power  \\\n","0  0.00000  0.490479  0.000000  0.000000  0.000000  0.000000  0.000000   \n","1  0.51742  0.000000  0.000000  0.000000  0.000000  0.393511  0.000000   \n","2  0.00000  0.000000  0.423394  0.423394  0.423394  0.322002  0.423394   \n","\n","    process     sampl   similar      test      text      use  \n","0  0.000000  0.490479  0.373022  0.490479  0.373022  0.00000  \n","1  0.000000  0.000000  0.393511  0.000000  0.393511  0.51742  \n","2  0.423394  0.000000  0.000000  0.000000  0.000000  0.00000  \n"]}]},{"cell_type":"code","source":["from sklearn.metrics.pairwise import cosine_similarity\n","\n","# Compute Cosine Similarity\n","cosine_sim_matrix = cosine_similarity(tfidf_matrix)\n","\n","# Convert the Cosine Similarity matrix to a DataFrame for better readability\n","cosine_sim_df = pd.DataFrame(cosine_sim_matrix, index=range(1, len(processed_documents_str) + 1), columns=range(1, len(processed_documents_str) + 1))\n","\n","# Display the Cosine Similarity Matrix\n","print(\"Cosine Similarity Matrix:\")\n","print(cosine_sim_df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o887XFfh27Eb","executionInfo":{"status":"ok","timestamp":1741586917116,"user_tz":-330,"elapsed":18,"user":{"displayName":"Parth Kumbhar","userId":"05674009928445710511"}},"outputId":"ffd392f1-d56f-488f-df7f-bc97cd44cccc"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Cosine Similarity Matrix:\n","          1         2         3\n","1  1.000000  0.293577  0.000000\n","2  0.293577  1.000000  0.126712\n","3  0.000000  0.126712  1.000000\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import jaccard_score\n","import numpy as np\n","\n","# Convert tokenized documents to binary bag-of-words representation\n","def binary_bag_of_words(documents, vocabulary):\n","    \"\"\"Convert tokenized documents into binary vectors based on a given vocabulary.\"\"\"\n","    return np.array([[1 if word in doc else 0 for word in vocabulary] for doc in documents])\n","\n","# Get unique words across all documents (vocabulary)\n","unique_words = set(word for doc in preprocessed_doc for word in doc)\n","\n","# Convert documents to binary bag-of-words format\n","binary_matrix = binary_bag_of_words(preprocessed_doc, unique_words)\n","\n","# Compute Jaccard Similarity for each pair of documents\n","num_docs = len(preprocessed_doc)\n","jaccard_sim_matrix = np.zeros((num_docs, num_docs))\n","\n","for i in range(num_docs):\n","    for j in range(num_docs):\n","        jaccard_sim_matrix[i][j] = jaccard_score(binary_matrix[i], binary_matrix[j])\n","\n","# Convert to a DataFrame for better readability\n","jaccard_sim_df = pd.DataFrame(\n","    jaccard_sim_matrix,\n","    index=range(1, num_docs + 1),\n","    columns=range(1, num_docs + 1)\n",")\n","\n","# Display the Jaccard Similarity Matrix\n","print(\"Jaccard Similarity Matrix:\")\n","print(jaccard_sim_df)\n"],"metadata":{"id":"bOzE_dx927xb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740723801108,"user_tz":-330,"elapsed":423,"user":{"displayName":"Parth Kumbhar","userId":"05674009928445710511"}},"outputId":"2fd55ad1-8d4b-4e00-d4f0-e0ba5761bdde"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Jaccard Similarity Matrix:\n","      1     2    3\n","1  1.00  0.25  0.0\n","2  0.25  1.00  0.1\n","3  0.00  0.10  1.0\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Y2hcuZUeHHEV"},"execution_count":null,"outputs":[]}]}